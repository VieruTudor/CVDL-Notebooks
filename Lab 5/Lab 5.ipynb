{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIlMQWUiUqxO"
   },
   "source": [
    "# Computer vision and deep learning - Laboratory 5\n",
    " \n",
    "In this laboratory we'll work with a semantic segmentation model. The task of semantic segmentation implies the labeling/classification of __all__ the pixels in the input image. \n",
    " \n",
    "You'll build and train a fully convolutional neural network inspired by U-Net. \n",
    "Also, you will learn about how you can use various callbacks during the training of your model.\n",
    " \n",
    "Finally, you'll implement several metrics suitable for evaluating segmentation models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYMNqbIyVx0R"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_ZDW2dzV67G"
   },
   "source": [
    "## Data loading\n",
    " \n",
    "As in the previous laboratory, we'll work with the OxfordPets dataset. Each image has a segmentation assigned; three classes are defined on each segmentation mask\n",
    "- Label 1: pet;\n",
    "- Label 3: border of the pet;\n",
    "- Label 2: background.\n",
    " \n",
    "Let's first write an offline processing step that will split our dataset into train/test sets, and will pre-process the input images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EO1_Wd_debx6"
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('images/Abyssinian_101.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "mask = cv2.imread('annotations/trimaps/Abyssinian_101.png', cv2.IMREAD_GRAYSCALE)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(mask)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(mask, cmap=\"gray\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XbPtem0gPfp"
   },
   "outputs": [],
   "source": [
    "mask = cv2.resize(mask, ( 45//2, 31//2), interpolation=cv2.INTER_NEAREST)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDvIwY2yytur"
   },
   "outputs": [],
   "source": [
    "def make_image_square(img, padding_mode='edge', padding_value=0):\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    if height > width:\n",
    "        padding = ((0, 0), ((height - width) // 2, (height - width) // 2), (0, 0))\n",
    "    else:\n",
    "        padding = (((width - height) // 2, (width - height) // 2), (0, 0), (0, 0))\n",
    "\n",
    "    if padding_mode == 'edge':\n",
    "        return np.pad(img, padding, mode=padding_mode)\n",
    "    return np.pad(img, padding, mode=padding_mode, constant_values=padding_value)\n",
    "\n",
    "\n",
    "def get_splits(root_dir):\n",
    "    image_paths = glob.glob(root_dir + \"/*.jpg\")\n",
    "    labels = [\"_\".join(os.path.basename(path).split(\"_\")[:-1]) for path in image_paths]\n",
    "    class_names = sorted(list(set(labels)))\n",
    "    assert (len(class_names) == 37)\n",
    "\n",
    "    X = image_paths\n",
    "    y = np.array([class_names.index(label) for label in labels])\n",
    "\n",
    "    X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=len(class_names)*20,\n",
    "                                                        random_state=13,\n",
    "                                                        shuffle=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def save_splits(images_paths, anno_dir, out_dir, img_shape = (128, 128)):\n",
    "    for img_path in images_paths:\n",
    "        filename = os.path.basename(img_path)\n",
    "        anno_path = os.path.join(anno_dir, filename.replace('.jpg', '.png'))\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print('Error while loading image: ', img_path)\n",
    "            continue\n",
    "        mask = cv2.imread(anno_path)\n",
    "\n",
    "        img = make_image_square(img, 'edge')\n",
    "        mask = make_image_square(mask, 'constant', 2) # 2 - color of the background pixels\n",
    "        img = cv2.resize(img, img_shape)\n",
    "        mask = cv2.resize(mask, img_shape, interpolation=cv2.INTER_NEAREST)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        np.save(os.path.join(out_dir, filename.replace('.jpg', '.img.npy')), img)\n",
    "        np.save(os.path.join(out_dir, filename.replace('.jpg', '.mask.npy')), mask)\n",
    "\n",
    "\n",
    "def save_db(images_dir, anno_dir, outdir, img_shape = (128, 128)):\n",
    "    X_train, _, X_test, _ = get_splits(images_dir)\n",
    "    train_dir = os.path.join(outdir, 'train')\n",
    "    test_dir = os.path.join(outdir, 'test')\n",
    "    if os.path.exists(outdir):\n",
    "        shutil.rmtree(outdir)\n",
    "    os.mkdir(outdir)\n",
    "    os.mkdir(train_dir)\n",
    "    os.mkdir(test_dir)\n",
    "\n",
    "    save_splits(X_train, anno_dir, train_dir)\n",
    "    save_splits(X_test, anno_dir, test_dir)\n",
    "\n",
    "\n",
    "def show_pair(img, mask):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title('Image')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask)\n",
    "    plt.title('Mask')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "save_db('./images', './annotations/trimaps', 'oxford_pets_seg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIoav3kjyzj4"
   },
   "source": [
    "You will use a slightly modified version of the data generator that you've written in the previous laboratory.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1bbpJCIywYQ"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, db_dir, batch_size,\n",
    "                 shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.image_paths, self.mask_paths = None, None\n",
    "        self.get_data(db_dir)\n",
    "        self.indices = np.arange(len(self.image_paths))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def get_data(self, root_dir):\n",
    "        \"\"\"\"\n",
    "        Loads the paths to the images and their corresponding labels from the database directory\n",
    "        \"\"\"\n",
    "        self.image_paths = np.asarray(glob.glob(root_dir + \"/*.img.npy\"))\n",
    "        self.mask_paths = np.asarray([path.replace('.img.npy', '.mask.npy') for path in self.image_paths])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of batches per epoch: the total size of the dataset divided by the batch size\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.image_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\"\n",
    "        Generates a batch of data\n",
    "        \"\"\"\n",
    "        batch_indices = self.indices[index*self.batch_size : (index+1)*self.batch_size]\n",
    "\n",
    "        batch_x = np.asarray([np.load(img_path).astype(np.float32)/255.0 for img_path in self.image_paths[batch_indices]])\n",
    "        batch_y = np.asarray([np.expand_dims(np.load(mask_path) - 1, axis=-1) for mask_path in self.mask_paths[batch_indices]])\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\"\n",
    "        Called at the end of each epoch\n",
    "        \"\"\"\n",
    "        # if required, shuffle your data after each epoch\n",
    "        self.indices = np.arange(len(self.image_paths))\n",
    "        if self.shuffle:\n",
    "            # you might find np.random.shuffle useful here\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "train_generator = DataGenerator(\"./oxford_pets_seg/train\", 32)\n",
    "\n",
    "batch_x, batch_y = train_generator[0]\n",
    "print(len(batch_x))\n",
    "fig, axes = plt.subplots(nrows=1, ncols=6, figsize=[16, 9])\n",
    "for i in range(len(axes)//2):\n",
    "    axes[2*i].set_title('Image')\n",
    "    axes[2*i].imshow(batch_x[i])\n",
    "\n",
    "    axes[2*i + 1].set_title('Mask')\n",
    "    axes[2*i + 1].imshow(batch_y[i][:,:,0]*64)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cNuZRCCWBQo"
   },
   "source": [
    "## Building the model\n",
    " \n",
    "The model that will be used in this laboratory is inspired by the [U-Net](https://arxiv.org/abs/1505.04597) architecture.\n",
    "U-Net is a fully convolutional neural network comprising two symmetric paths: a contracting path (to capture context) and an expanding path  (which enables precise localization). \n",
    "The network also uses skip connections between the corresponding layers in the downsampling path to the layer in the upsampling path, and thus directly fast-forwards high-resolution feature maps from the encoder to the decoder network.\n",
    " \n",
    "An overview of the U-Net architecture is depicted in the figure below.\n",
    " \n",
    "<img src=\"https://miro.medium.com/max/1400/1*J3t2b65ufsl1x6caf6GiBA.png\"/>\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKKi5UowzvjZ"
   },
   "source": [
    "## The downsampling path\n",
    " \n",
    "For the downsampling path we'll use a convolutional neural network from the tensorflow.keras.applications module. We'll first load the pre-trained weights on ImageNet, and we'll \"freeze\" these weights during the training process.\n",
    "In this example, we'll use the Mobile-Net architecture, but you can feel free to experiment with any other network.\n",
    " \n",
    "The problem is that to create the skip connections required by the U-Net architecture we need access to the feature maps of some intermediate layers in the network and these are not accessible by default.\n",
    " \n",
    "The first step is to determine the names for the layers that will be included in the skip connections. So, let's load the MobileNetV2 architecture and use the _summary()_ method to identify these layers.\n",
    "[MobileNetV2](https://keras.io/api/applications/) has a size of 16MB and achieves a top-1 accuracy of 71.3% on ImageNet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0beF9vMCWH14"
   },
   "outputs": [],
   "source": [
    "input_shape = (128, 128, 3)\n",
    "feature_extractor = tf.keras.applications.MobileNetV2(input_shape=input_shape, include_top=False)\n",
    "feature_extractor.summary()\n",
    "\n",
    "# TODO select the names of the layers that we'll be included in the skip connections\n",
    "downsample_skip_layers = [\"block_1_expand_relu\", \"block_2_expand_relu\", \"block_5_expand_relu\", \"block_8_expand_relu\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TduQa06Fgyju"
   },
   "source": [
    "After identifying the layers that you want to use in the skip connections, you can get access to these layers by using the method _get\\_layer()_ defined from the _tensorflow.keras.Model_ class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogtIj9jqh1Gl"
   },
   "outputs": [],
   "source": [
    "layer_act_map = feature_extractor.get_layer(\"block_1_expand_relu\").output\n",
    "print(layer_act_map)\n",
    "\n",
    "for layer_name in downsample_skip_layers:\n",
    "  print(layer_name, '->', feature_extractor.get_layer(layer_name).output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a46ZnuuTzzR6"
   },
   "source": [
    "## The upsampling path\n",
    " \n",
    "In the upsampling path, we'll use transposed convolutions to progressively increase the resolution of the activation maps. The layers for the transposed convolution is [Conv2DTranspose](https://keras.io/api/layers/convolution_layers/convolution2d_transpose/).\n",
    " \n",
    "Let's write a function to implement an upsampling block, consisting of a transposed convolution, a batch normalization block and a ReLu activation.\n",
    " \n",
    "Remember, the output size $W_o$ of a transposed convolutional layer is:  \n",
    "\\begin{equation}\n",
    "W_o = (W_i - 1) \\cdot S - 2P + F\n",
    "\\end{equation},\n",
    " \n",
    "where $W_i$ is the size of the input, $S$ is the stride, $P$ is the amount of padding and $F$ is the filter size.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCh0txX24w2i"
   },
   "outputs": [],
   "source": [
    "def upsample_block(x, filters, size, stride = 2):\n",
    "  \"\"\"\n",
    "  x - the input of the upsample block\n",
    "  filters - the number of filters to be applied\n",
    "  size - the size of the filters\n",
    "  \"\"\"\n",
    "  x = tf.keras.layers.Conv2DTranspose(filters, size, strides=(stride, stride), padding='same', kernel_initializer='he_normal')(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.layers.ReLU()(x)\n",
    "  return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOg9FKZmj7dH"
   },
   "source": [
    "Now let's test this upsampling block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDy7Qkzmj6Tv"
   },
   "outputs": [],
   "source": [
    "in_layer = feature_extractor.get_layer(downsample_skip_layers[0]).output\n",
    "\n",
    "filter_sz = 3\n",
    "num_filters = 16\n",
    "\n",
    "for stride in [2, 4, 8]:\n",
    "  x = upsample_block(in_layer, num_filters, filter_sz, stride)\n",
    "  print('in shape: ', in_layer.shape, ' upsample with filter size ', filter_sz, '; stride ', stride, ' -> out shape ', x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGYvtdTLz2_A"
   },
   "source": [
    "## Putting it all together\n",
    " \n",
    "Now we understand all the parts required to build the U-Net architecture.\n",
    " \n",
    "Let's write the function _build\\_unet()_ which will build our architecture. Of course, we'll use the function API when writing the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhQn5hPSnJAQ"
   },
   "outputs": [],
   "source": [
    "def build_unet(input_shape, num_classes):\n",
    "  # define the input layer\n",
    "  inputs = None\n",
    "  inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "  # TODO define the downsampling path as a MobilenetV2 architecture, pre-loaded with imagenet weights\n",
    "  mobilenet_v2 = tf.keras.applications.MobileNetV2(input_shape=input_shape, include_top=False)\n",
    "      \n",
    "  downsample_skip_layer_name = [\"block_1_expand_relu\", \"block_2_expand_relu\", \"block_5_expand_relu\", \"block_8_expand_relu\"]\n",
    "\n",
    "  # get access to the skip layers in the downsample path\n",
    "  down_stack = tf.keras.Model(inputs=mobilenet_v2.input, outputs=[mobilenet_v2.get_layer(name).output for name in downsample_skip_layer_name]) \n",
    "  # TODO freeze the downsampling path\n",
    "  down_stack.trainable = False\n",
    "  \n",
    "\n",
    "  skips = down_stack(inputs)\n",
    "  x = skips[-1]\n",
    "\n",
    "  filter_sz = 3\n",
    "  for skip_layer in  reversed(skips[:-1]):\n",
    "    # determine the number of filters\n",
    "    filters = skip_layer.shape[-1]\n",
    "    x = upsample_block(x, filters, filter_sz)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip_layer])\n",
    "    \n",
    "  # add the last conv2d transpose layer with the number of filters equal to the number of segmentation classes\n",
    "  output = tf.keras.layers.Conv2DTranspose(num_classes, filter_sz, strides=(2, 2), padding='same')(x)\n",
    "\n",
    "  # return the model\n",
    "  return tf.keras.Model(inputs=inputs, outputs=output)\n",
    "  \n",
    "model = build_unet((128, 128, 3), 3)\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxKHMWHrWITo"
   },
   "source": [
    "##  Training the model. Defining callbacks. \n",
    " \n",
    "Now that we've built the model, we can proceed to the training step.\n",
    "We are dealing with a multi-class segmentation problem (pet pixels, pet border pixels and background pixels), so we should use the  [tf.keras.losses.CategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy) loss function.\n",
    " \n",
    "When training the model you can use various [callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback).\n",
    "Define the following callbacks:\n",
    "- [ModelCheckpoint](https://keras.io/api/callbacks/model_checkpoint/) to save the best model so far every 5 to 5 epochs;\n",
    "- [TerminateOnNaN](https://keras.io/api/callbacks/terminate_on_nan/) to stop the training process is a NaN loss is encountered;\n",
    "- [EarlyStopping](https://keras.io/api/callbacks/early_stopping/) for when the loss has stopped improving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRbaJtCmxsHZ"
   },
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(\"./oxford_pets_seg/train\", 32)\n",
    "val_generator = DataGenerator(\"./oxford_pets_seg/test\", 32)\n",
    "\n",
    "def train(input_shape, num_classes, train_datagen, val_datagen, num_epochs=5):\n",
    "  model = build_unet(input_shape, num_classes)\n",
    "  # TODO configure the traning process model.compile(...)\n",
    "  model.compile(\n",
    "      # TODO params\n",
    "      optimizer=\"adam\",\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=\"auto\", name=\"sparse_categorical_crossentropy\"\n",
    "      ),\n",
    "      metrics=[\"accuracy\"]\n",
    "  )\n",
    "  \n",
    "  # train the model \n",
    "  history = model.fit(train_datagen, validation_data=val_datagen, epochs=num_epochs)\n",
    "  model.save('model_final.h5')\n",
    "\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  plt.figure()\n",
    "  plt.plot(history.epoch, loss, 'r', label='Training loss')\n",
    "  plt.plot(history.epoch, val_loss, 'b', label='Validation loss')\n",
    "  plt.title('Loss evolution')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.ylim([0, 1])\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "  return model\n",
    "\n",
    "model = train((128, 128, 3), 3, train_generator, val_generator, 5)\n",
    "\n",
    "# now let's see some predictions\n",
    "batch_x, batch_y = val_generator[0]\n",
    "predictions = model.predict(batch_x)\n",
    "predictions = tf.argmax(predictions, axis=-1)\n",
    "fig, axes = plt.subplots(nrows=6, ncols=3, figsize=[16, 9])\n",
    "\n",
    "for i in range(len(axes)):\n",
    "    axes[i][0].imshow(batch_x[i])\n",
    "    axes[i][0].set_title('Image')\n",
    "\n",
    "    axes[i][1].imshow(batch_y[i][:, :, 0])\n",
    "    axes[i][1].set_title('GT Mask')\n",
    "\n",
    "    axes[i][2].imshow(predictions[i] )\n",
    "    axes[i][2].set_title('Pred Mask')\n",
    "    \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHwneOq3WCxA"
   },
   "source": [
    "## Evaluation metrics\n",
    " \n",
    "Finally, you will implement several segmentation metrics to evaluate the model you've just trained. As usual, try to implement these metrics without using any for loops.\n",
    " \n",
    "In the remainder of this section we'll use the following notation:\n",
    "- $n_{ij}$ - the total number of pixels classified to class\n",
    "j but actually belonging to class i; $i, j \\in 1, .., C$;\n",
    "- $t_i = \\sum_{j = 1}^{C} n_{ij}$ - the total number of pixels belonging to class $i$ (in the ground truth segmentation mask);\n",
    "- $C$ - the total number of classes in the segmentation problem.\n",
    " \n",
    "### Mean pixel accuracy\n",
    " \n",
    "Pixel accuracy is the simplest image segmentation metric; it is defined as the percentage of pixels that were correctly classified by the model.\n",
    " \n",
    "\\begin{equation}\n",
    "p_a = \\frac{1}{C} \\frac{\\sum_{i}^{C} n_{ii}}{\\sum_{i}^{C} t_i} \n",
    "\\end{equation}\n",
    " \n",
    "This metric is not that relevant for class imbalanced problems (which occurs for most segmentation problems).\n",
    " \n",
    "### Intersection over Union (IoU)\n",
    " \n",
    "the intersection over union metric is defined as the ratio between the area of intersection and the area of union (between the predicted segmentation mask and the ground truth segmentation mask of a single class).\n",
    "In case of a multi-class segmentation problem, we simple average the IoUs over all the classes. This metric is called mean Intersection over Union (mIou).\n",
    " \n",
    "\\begin{equation}\n",
    "mIoU = \\frac{1}{C} \\sum_{i = 1}^{C} \\frac{n_{ii}}{t_i - n_{ii} + \\sum_{j = 1}^{C} n_{ji}}\n",
    "\\end{equation}\n",
    " \n",
    "The ideal value for this metric is 1; usually values lower than 0.6 indicate a very bad performance.\n",
    " \n",
    "### Frequency Weighted Intersection over Union\n",
    " \n",
    "The frequency weighted over union metric is similar to mean IoU, but the values are weighted with the adequate frequencies of the pixels.\n",
    " \n",
    "\\begin{equation}\n",
    "fIou = (\\sum_{i = 1}^{C} t_i)^{-1}   \\sum_{i = 1}^{C} t_i \\cdot \\frac{n_{ii}}{t_i - n_{ii} + \\sum_{j = 1}^{C} n_{ji}}\n",
    "\\end{equation}\n",
    " \n",
    "The values of this metric lie in the interval [0, 1], and the ideal value for this metric is 1.\n",
    " \n",
    "Compute and report these metrics for your trained model(s).\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oz3PPFsjqylX"
   },
   "outputs": [],
   "source": [
    "def mean_pixel_acc(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  y_true - array of shape (batch_size, height, width) with the ground truth labels\n",
    "  y_pred - array of shape (bacth_size, height, width) with the predicted labels \n",
    "  \"\"\"\n",
    "  eq = np.equal(y_true, y_pred)\n",
    "  return np.mean(eq)\n",
    "\n",
    "def get_intersection_and_union(true, pred):\n",
    "    cm = tf.math.confusion_matrix(true, pred, num_classes=3, dtype=tf.dtypes.float64)\n",
    "    intersection = tf.linalg.diag_part(cm)\n",
    "    union = tf.reduce_sum(cm, axis=1) - intersection + tf.reduce_sum(cm, axis=0)\n",
    "    return intersection, union\n",
    "\n",
    "def iou(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  y_true - array of shape (batch_size, height, width) with the ground truth labels\n",
    "  y_pred - array of shape (bacth_size, height, width) with the predicted labels \n",
    "  \"\"\"\n",
    "  flat = tf.keras.layers.Flatten()\n",
    "  flatten_true = flat(y_true)\n",
    "  flatten_pred = flat(y_pred)\n",
    "  intersection, union = tf.map_fn(lambda x: get_intersection_and_union(x[0], x[1]), (flatten_true, flatten_pred), fn_output_signature=(tf.float64, tf.float64))\n",
    "  mean = tf.reduce_mean(tf.math.divide_no_nan(intersection, union), axis=1)\n",
    "  return np.mean(mean)\n",
    "\n",
    "def get_intersection_and_union_weighted(true, pred):\n",
    "    cm = tf.math.confusion_matrix(true, pred, num_classes=3, dtype=tf.dtypes.float64)\n",
    "    intersection = tf.linalg.diag_part(cm)\n",
    "    total_pixels =  tf.reduce_sum(cm, axis=1)\n",
    "    union = total_pixels - intersection + tf.reduce_sum(cm, axis=0)\n",
    "    return intersection*total_pixels, union\n",
    "\n",
    "def fw_iou(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  y_true - array of shape (batch_size, height, width) with the ground truth labels\n",
    "  y_pred - array of shape (bacth_size, height, width) with the predicted labels \n",
    "  \"\"\"\n",
    "  # TODO your code here\n",
    "  height = y_true.shape[1]\n",
    "  width = y_true.shape[2]\n",
    "  flat = tf.keras.layers.Flatten()\n",
    "  flatten_true = flat(y_true)\n",
    "  flatten_pred = flat(y_pred)\n",
    "  intersection, union = tf.map_fn(lambda x: get_intersection_and_union_weighted(x[0], x[1]), (flatten_true, flatten_pred), fn_output_signature=(tf.float64, tf.float64))\n",
    "  mean = tf.reduce_sum(tf.math.divide_no_nan(intersection, union), axis=1) / (height*width)\n",
    "  return np.mean(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_x = []\n",
    "B_y = []\n",
    "preds = []\n",
    "for batch_x, batch_y in val_generator:\n",
    "    B_x.extend(batch_x)\n",
    "    B_y.extend(batch_y)\n",
    "B_x = np.asarray(B_x)\n",
    "B_y = np.asarray(B_y)\n",
    "B_y = B_y[:, :, :, 0].astype(np.int64)\n",
    "predictions = model.predict(B_x)\n",
    "predictions = tf.argmax(predictions, axis=-1).numpy()\n",
    "print(f\"mean pixel acc: {mean_pixel_acc(B_y, predictions)}\")\n",
    "print(f\"iou: {iou(B_y, predictions)}\")\n",
    "print(f\"fw_iou: {fw_iou(B_y, predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Computer Vision and Deep Learning - Laboratory 5 [students].ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a84fd92db5a471e91f372cecb47adab54dcdef527b642916ca90b59569abdda0"
  },
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
