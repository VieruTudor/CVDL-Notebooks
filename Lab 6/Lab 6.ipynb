{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1IfPVkblsYn"
   },
   "source": [
    "# Computer Vision and Deep Learning - Laboratory 6\n",
    " \n",
    "Congratulations, you made it till the last laboratory of the semester. This laboratory will be a bit different: it contains two parts and you can __choose__ at your own preference which of them you want to solve.  Of course, you can choose to solve both of them.\n",
    " \n",
    "The first part is related to _visualization_, while the second one is related to sequence models and vision transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73eaDA20lr9T"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1qTJrNimS9x"
   },
   "source": [
    "# Part 1. Visualizing what neural networks learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idxv04YymnlB"
   },
   "source": [
    "For this part you can either work with a neural network that you trained (perhaps for your project), or with a pre-trained model from tensorflow.\n",
    " \n",
    "For illustration purposes, I will load the Resnet network, pre-trained on Imagenet and, of course, an image of a cat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O cat.jpg 'https://img.freepik.com/free-photo/cat-white-background_155003-20502.jpg?size=626&ext=jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "jN_opFA5pYcK",
    "outputId": "51bcadfc-dfb0-40bf-ac72-54dfce4d8d5c"
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('cat.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "img = cv2.resize(img, (224, 224))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5p35id8ToLcJ"
   },
   "outputs": [],
   "source": [
    "resnet50_model = ResNet50(weights='imagenet')\n",
    "resnet50_model.summary()\n",
    "#tf.keras.utils.plot_model(resnet50_model, show_shapes=True, show_dtype=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4vW2mLKBTyE"
   },
   "source": [
    "## Display the filters in the first and second convolutional layer\n",
    "\n",
    "Get the filters in the first and second convolutional layers and plot them. The filters in the first convolutional layer should be displayed as color images, while for the filters in the second layer you should display each channel individually as a grayscale image.\n",
    "\n",
    "Identify the names of these two layers and then use _layer.get_weights()_ to access the values of the filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miO5i9diCGsZ"
   },
   "outputs": [],
   "source": [
    "layer1_weights = resnet50_model.get_layer(\"conv1_conv\").get_weights()[0]\n",
    "mn = np.min(layer1_weights)\n",
    "layer1_weights -= mn\n",
    "mx = np.max(layer1_weights)\n",
    "layer1_weights /= mx\n",
    "layer1_weights *= 255\n",
    "layer1_weights = layer1_weights.astype(int)\n",
    "fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "for i in range(64):\n",
    "    plt.subplot(8, 8, i+1)\n",
    "    plt.imshow(layer1_weights[:, :, :, i])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_weights = resnet50_model.get_layer(\"conv2_block1_1_conv\").get_weights()[0].reshape((8, 8, 64))\n",
    "mn = np.min(layer1_weights)\n",
    "layer1_weights -= mn\n",
    "mx = np.max(layer1_weights)\n",
    "layer1_weights /= mx\n",
    "layer1_weights *= 255\n",
    "layer1_weights = layer1_weights.astype(int)\n",
    "fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "for i in range(64):\n",
    "    plt.subplot(8, 8, i+1)\n",
    "    plt.imshow(layer1_weights[:, :, i], cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6AQsNvsnjxS"
   },
   "source": [
    "## Saliency map via image occlusions\n",
    " \n",
    "This visualization technique will output a heatmap which will highlight the regions that the model finds important when predicting a certain class.\n",
    " \n",
    "The implementation is straightforward: you just slide an occluding patch over the input image, and, for each position of the patch, feed the occluded image to the network and store the predictions (probas) of the model.\n",
    " \n",
    "Finally, display (as a heatmap) the probability of the correct class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70Kp9WYOoKr8"
   },
   "outputs": [],
   "source": [
    "def compute_saliency_map(model, img, patch_size, stride):\n",
    "    \"\"\"\n",
    "    Succesivelly occlude the input image with a gray square patch of size patch_size.\n",
    "    When sliding the patch over the input image a step equal to stride is used.\n",
    "    \"\"\"\n",
    "\n",
    "    padded_image = np.pad(img, ((patch_size//2, patch_size//2), (patch_size//2, patch_size//2), (0, 0)), mode=\"edge\")\n",
    "\n",
    "    # TODO your code here: compute the size of the output\n",
    "    wo = int((img.shape[1] + 2*(patch_size//2))/stride + 1) # the width of the output heatmap\n",
    "    ho = int((img.shape[0] + 2*(patch_size//2))/stride + 1) # the height of the output heatmap\n",
    "    heatmap = np.zeros((ho, wo), dtype=np.float32)\n",
    "\n",
    "    true_class = tf.keras.backend.get_value(model(np.expand_dims(tf.keras.applications.resnet50.preprocess_input(img), 0)))\n",
    "    print(tf.keras.applications.resnet50.decode_predictions(true_class))\n",
    "    true_class = np.argmax(true_class, axis=-1)[0]\n",
    "    batch = []\n",
    "\n",
    "    for y in range(0, ho, stride):\n",
    "        for x in range(0, wo, stride):\n",
    "            patched_image = np.copy(img)\n",
    "            patched_image[y*stride-patch_size//2:y*stride+patch_size//2,x*stride-patch_size//2:x*stride+patch_size//2, :] = 128\n",
    "            batch.append(patched_image)\n",
    "    pred = model.predict(np.asarray(batch), batch_size=32)\n",
    "    print(pred.shape)\n",
    "    print(true_class)\n",
    "    scores = pred[:, true_class]\n",
    "    heatmap = np.reshape(scores, (int(np.sqrt(scores.shape[0])), int(np.sqrt(scores.shape[0]))))\n",
    "    return heatmap\n",
    "\n",
    "heatmap = compute_saliency_map(resnet50_model, img, 32, 4)\n",
    "# mn = np.min(heatmap)\n",
    "# mx = np.max(heatmap)\n",
    "# heatmap -= mn\n",
    "# heatmap /= (mx-mn)\n",
    "# heatmap *= 255\n",
    "# heatmap = heatmap.astype(int)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(heatmap)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QVtwSjSeloe"
   },
   "source": [
    "## Saliency maps via image derivative\n",
    " \n",
    "This type of visualization tries to determine the pixels that contributed the most in the final classification.\n",
    "The main idea is to compute the derivative of the scores with respect to the input image. This derivative can be seen as a class saliency map for the input image (its magnitude tells us what pixels should be modified to change the class score the most).\n",
    " \n",
    "You can use a [GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) to compute the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "AnbU9z04hALW",
    "outputId": "a429cb18-085e-4f00-8ff0-cce9d7729597"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "def compute_saliency_map(model, img):\n",
    "  \n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "\n",
    "    img = tf.Variable(img, dtype=float)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(img, training=False)\n",
    "        loss = pred[0, np.argmax(pred, axis=-1)[0]]\n",
    "        grads = tape.gradient(loss, img)\n",
    "        abs_grad = np.abs(grads)\n",
    "        max_abs_grad = np.max(abs_grad, axis=3)\n",
    "\n",
    "        # TODO your code here: normalize to range between 0 and 1\n",
    "        mn = np.min(max_abs_grad)\n",
    "        grad_norm = (max_abs_grad - mn)/(np.max(max_abs_grad) - mn)\n",
    "    return grad_norm\n",
    "\n",
    "\n",
    "model = tf.keras.applications.mobilenet_v2.MobileNetV2()\n",
    "saliency_map = compute_saliency_map(model, img)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14,5))\n",
    "ax[0].imshow(img)\n",
    "ax[1].imshow(saliency_map[0, :, :],cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwzWevVDhAz_"
   },
   "source": [
    "## t-SNE visualization (optional)\n",
    "\n",
    "You can use the ImageNetes Apply t-SNE visualization on the last layer of the network that you chose.\n",
    "You can use the [T-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) module from sklean library.\n",
    "\n",
    "To create this visualization, you will need several images and their ground truth class. You could use some images from [imagenette](https://github.com/fastai/imagenette).\n",
    "\n",
    "Feed the images through your model and save the activation maps of the layer just before the classification layer (create a new model starting from the pre-trained architecture that you used and set that layer as the input). Then, use t-SNE visualization to view the classes (the colors of the points should be determined by the class of those images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOLWatg-sb5g"
   },
   "source": [
    "# Deep-dream\n",
    "\n",
    "Follow this [tutorial](https://www.tensorflow.org/tutorials/generative/deepdream) related to DeepDream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81aB7Dkisbbn"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "import IPython.display as display\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg'\n",
    "# Download an image and read it into a NumPy array.\n",
    "def download(url, max_dim=None):\n",
    "  name = url.split('/')[-1]\n",
    "  image_path = tf.keras.utils.get_file(name, origin=url)\n",
    "  img = PIL.Image.open(image_path)\n",
    "  if max_dim:\n",
    "    img.thumbnail((max_dim, max_dim))\n",
    "  return np.array(img)\n",
    "\n",
    "# Normalize an image\n",
    "def deprocess(img):\n",
    "  img = 255*(img + 1.0)/2.0\n",
    "  return tf.cast(img, tf.uint8)\n",
    "\n",
    "# Display an image\n",
    "def show(img):\n",
    "  display.display(PIL.Image.fromarray(np.array(img)))\n",
    "\n",
    "\n",
    "# Downsizing the image makes it easier to work with.\n",
    "original_img = download(url, max_dim=500)\n",
    "show(original_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximize the activations of these layers\n",
    "names = ['mixed3', 'mixed5']\n",
    "layers = [base_model.get_layer(name).output for name in names]\n",
    "\n",
    "# Create the feature extraction model\n",
    "dream_model = tf.keras.Model(inputs=base_model.input, outputs=layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(img, model):\n",
    "  # Pass forward the image through the model to retrieve the activations.\n",
    "  # Converts the image into a batch of size 1.\n",
    "  img_batch = tf.expand_dims(img, axis=0)\n",
    "  layer_activations = model(img_batch)\n",
    "  if len(layer_activations) == 1:\n",
    "    layer_activations = [layer_activations]\n",
    "\n",
    "  losses = []\n",
    "  for act in layer_activations:\n",
    "    loss = tf.math.reduce_mean(act)\n",
    "    losses.append(loss)\n",
    "\n",
    "  return  tf.reduce_sum(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepDream(tf.Module):\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "\n",
    "  @tf.function(\n",
    "      input_signature=(\n",
    "        tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[], dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=[], dtype=tf.float32),)\n",
    "  )\n",
    "  def __call__(self, img, steps, step_size):\n",
    "      print(\"Tracing\")\n",
    "      loss = tf.constant(0.0)\n",
    "      for n in tf.range(steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "          # This needs gradients relative to `img`\n",
    "          # `GradientTape` only watches `tf.Variable`s by default\n",
    "          tape.watch(img)\n",
    "          loss = calc_loss(img, self.model)\n",
    "\n",
    "        # Calculate the gradient of the loss with respect to the pixels of the input image.\n",
    "        gradients = tape.gradient(loss, img)\n",
    "\n",
    "        # Normalize the gradients.\n",
    "        gradients /= tf.math.reduce_std(gradients) + 1e-8 \n",
    "\n",
    "        # In gradient ascent, the \"loss\" is maximized so that the input image increasingly \"excites\" the layers.\n",
    "        # You can update the image by directly adding the gradients (because they're the same shape!)\n",
    "        img = img + gradients*step_size\n",
    "        img = tf.clip_by_value(img, -1, 1)\n",
    "\n",
    "      return loss, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepdream = DeepDream(dream_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_deep_dream_simple(img, steps=100, step_size=0.01):\n",
    "  # Convert from uint8 to the range expected by the model.\n",
    "  img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "  img = tf.convert_to_tensor(img)\n",
    "  step_size = tf.convert_to_tensor(step_size)\n",
    "  steps_remaining = steps\n",
    "  step = 0\n",
    "  while steps_remaining:\n",
    "    if steps_remaining>100:\n",
    "      run_steps = tf.constant(100)\n",
    "    else:\n",
    "      run_steps = tf.constant(steps_remaining)\n",
    "    steps_remaining -= run_steps\n",
    "    step += run_steps\n",
    "\n",
    "    loss, img = deepdream(img, run_steps, tf.constant(step_size))\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    show(deprocess(img))\n",
    "    print (\"Step {}, loss {}\".format(step, loss))\n",
    "\n",
    "\n",
    "  result = deprocess(img)\n",
    "  display.clear_output(wait=True)\n",
    "  show(result)\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dream_img = run_deep_dream_simple(img=original_img, \n",
    "                                  steps=100, step_size=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "OCTAVE_SCALE = 1.30\n",
    "\n",
    "img = tf.constant(np.array(original_img))\n",
    "base_shape = tf.shape(img)[:-1]\n",
    "float_base_shape = tf.cast(base_shape, tf.float32)\n",
    "\n",
    "for n in range(-2, 3):\n",
    "  new_shape = tf.cast(float_base_shape*(OCTAVE_SCALE**n), tf.int32)\n",
    "\n",
    "  img = tf.image.resize(img, new_shape).numpy()\n",
    "\n",
    "  img = run_deep_dream_simple(img=img, steps=50, step_size=0.01)\n",
    "\n",
    "display.clear_output(wait=True)\n",
    "img = tf.image.resize(img, base_shape)\n",
    "img = tf.image.convert_image_dtype(img/255.0, dtype=tf.uint8)\n",
    "show(img)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Computer Vision and Deep Learning - Laboratory 6.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a84fd92db5a471e91f372cecb47adab54dcdef527b642916ca90b59569abdda0"
  },
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
