{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9eHiescHCkV"
   },
   "source": [
    "# Computer Vision and Deep Learning - Laboratory 4\n",
    " \n",
    "The main objective of this laboratory is to familiarize you with the training process of a neural network. More specifically, you'll follow this [\"recipe\"](!http://karpathy.github.io/2019/04/25/recipe/) for training  neural networks proposed by Andrew Karpathy.\n",
    "You'll go through all the steps of training, data preparation, debugging, hyper-parameter tuning.\n",
    " \n",
    "In the second part of the laboratory, you'll experiment with _transfer learning_ and _fine-tuning_.  Transfer learning is a concept from machine learning which allows you to reuse the knowledge gained while solving a problem (in our case the CNN weights) and applying it to solve a similar problem. This is useful when you are facing a classification problem with a small training dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fk1OiQhNRwj-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import threading\n",
    "import cv2.cv2 as cv2\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnyWBvH6RuF3"
   },
   "source": [
    "# Data loading. Training a neural network. Tuning hyper-parameters. \n",
    "\n",
    "Your task for the first part of the laboratory is to train a convolutional nerual network for image classification. You can choose any dataset for image classification. By default you can use the [Oxford Pets dataset](!https://www.robots.ox.ac.uk/~vgg/data/pets/), but you can choose a dataset that you will be using for your project or an interesting dataset from [Kaggle](!https://www.kaggle.com/datasets?search=image).\n",
    "\n",
    "So the first step would be download your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zbn5ScvTsMH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !curl https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz -o images.tar.gz # replace it with the link to the dataset that you will be using\n",
    "# !curl https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz -o annotations.tar.gz\n",
    "\n",
    "# !tar -xvf images.tar.gz\n",
    "# !tar -xvf annotations.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de1M08vUTxCc"
   },
   "source": [
    "## Data loading \n",
    " \n",
    "Up until now, we could load the data to train our model in a single line of code: we just used numpy.load to read the entire training and test sets into memory.\n",
    "However, in some cases we won't be able to fit all the data into the memory due to hardware constraints.\n",
    " \n",
    "To alleviate this problem, we'll use the [_Sequence_](!https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence) class from tensorflow which allows us to feed data to our models.\n",
    "To write a custom data generator, you'll have to \n",
    "- write a class that inherits from the class _Sequence_\n",
    "- override the \\_\\_len\\_\\_ method: this method should return the number of batches in a sequence. In this method you can just return the value:\n",
    "\\begin{equation}\n",
    "len = \\frac{training\\_samples}{batch\\_size}\n",
    "\\end{equation}\n",
    "- override the \\_\\_get_item\\_\\_(self, index) method: this should return a complete batch;\n",
    "- optionally, you can override other methods, such as on_epoch_end(). For example, here you could shuffle the data after each epoch.\n",
    " \n",
    "What's nice about this is that when calling the fit() method on a model with a _Sequence_, you can set the use_multiprocessing to True and use several workers that will generate the training batches in parallel.\n",
    " \n",
    "``\n",
    "fit(\n",
    "    x=None, y=None, batch_size=None, epochs=1, verbose='auto',\n",
    "    callbacks=None, validation_split=0.0, validation_data=None, shuffle=True,\n",
    "    class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None,\n",
    "    validation_steps=None, validation_batch_size=None, validation_freq=1,\n",
    "    max_queue_size=10, workers=1, use_multiprocessing=False\n",
    ")\n",
    "``\n",
    " \n",
    "Start by writing a custom data generator for the dataset that you chose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9F51XaQQSdy"
   },
   "outputs": [],
   "source": [
    "import cv2.cv2 as cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, labels_file, label_names_file, batch_size, input_size, shuffle=True):\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.class_names = {}\n",
    "        with open(label_names_file) as f:\n",
    "            for line in f.readlines():\n",
    "                class_id, name = line[:-1].split(\",\")\n",
    "                self.class_names[int(class_id)] = name\n",
    "        self.num_classes = len(self.class_names)\n",
    "        self.data, self.labels = self.get_data(labels_file)\n",
    "        self.indices = np.arange(len(self.data))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def get_data(self, root_dir):\n",
    "        \"\"\"\"\n",
    "        Loads the paths to the images and their corresponding labels from the database directory\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        with open(root_dir) as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                path, class_id = line[:-1].split(\",\")\n",
    "                self.data.append(path)\n",
    "                self.labels.append(int(class_id))\n",
    "        return self.data, np.asarray(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of batches per epoch: the total size of the dataset divided by the batch size\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\"\n",
    "        Generates a batch of data\n",
    "        \"\"\"\n",
    "        batch_indices = self.indices[index * self.batch_size: min(len(self.indices), (index + 1) * self.batch_size)]\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        for i in batch_indices:\n",
    "            image = cv2.imread(self.data[i])\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = DataGenerator.resize_image(image, self.input_size)\n",
    "            image = image / 255.0\n",
    "            batch_x.append(image)\n",
    "            batch_y.append(self.labels[i])\n",
    "        return np.asarray(batch_x), np.asarray(batch_y)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\"\n",
    "        Called at the end of each epoch\n",
    "        \"\"\"\n",
    "        self.indices = np.arange(len(self.data))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_image(image):\n",
    "        width_pad = 0\n",
    "        height_pad = 0\n",
    "        if image.shape[0] > image.shape[1]:\n",
    "            width_pad = (image.shape[0] - image.shape[1]) // 2\n",
    "        else:\n",
    "            height_pad = (image.shape[1] - image.shape[0]) // 2\n",
    "        return np.pad(image, ((height_pad, height_pad), (width_pad, width_pad), (0, 0)), mode=\"edge\")\n",
    "\n",
    "    @staticmethod\n",
    "    def resize_image(image, shape):\n",
    "        image = DataGenerator.pad_image(image)\n",
    "        return cv2.resize(image, shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxN9GrN8Q0PA"
   },
   "source": [
    "Now let's look at some images and samples from our data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C654AGSCzFx5"
   },
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(f\"data/train.csv\", f\"data/classes.csv\", 32, (32, 32))\n",
    "label_names = train_generator.class_names\n",
    "batch_x, batch_y = train_generator[5]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=8, figsize=[16, 9])\n",
    "for i in range(axes.shape[0]):\n",
    "    for j in range(axes.shape[1]):\n",
    "        axes[i][j].set_title(label_names[batch_y[i*axes.shape[1]+j]])\n",
    "        axes[i][j].imshow(batch_x[i*axes.shape[1]+j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kd0tRSjP2jZL"
   },
   "source": [
    "# CNN architecture\n",
    "\n",
    "Write a simple tensorflow architecture for a convolutional neural network.\n",
    "Use the [functional](!https://www.tensorflow.org/guide/keras/functional) api when writing the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kx36t4ug3H_B"
   },
   "outputs": [],
   "source": [
    "OUTPUTS = 1\n",
    "with open(\"data/classes.csv\") as f:\n",
    "    for line in f.readlines():\n",
    "        class_id = int(line[:-1].split(\",\")[0])\n",
    "        OUTPUTS = max(OUTPUTS, class_id+1)\n",
    "print(OUTPUTS)\n",
    "INPUT_SHAPE = (64, 64)\n",
    "INPUT_SHAPE_RGB = (*INPUT_SHAPE, 3)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "def generate_test_train():\n",
    "    with open(\"data/photos.csv\", \"r\") as file:\n",
    "        with open(\"data/test.csv\", \"w\") as test:\n",
    "            with open(\"data/train.csv\", \"w\") as train:\n",
    "                for line in file.readlines():\n",
    "                    if random.random() < 0.2:\n",
    "                        test.write(line)\n",
    "                    else:\n",
    "                        train.write(line)\n",
    "\n",
    "\n",
    "def resnet_block(input_layer, filter_size=3, no_filters=16):\n",
    "    layer1 = layers.Conv2D(kernel_size=filter_size, filters=no_filters, padding=\"same\", activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(input_layer)\n",
    "    layer2 = layers.Conv2D(kernel_size=filter_size, filters=no_filters, padding=\"same\", activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(layer1)\n",
    "    return layers.Add()([input_layer, layer2])\n",
    "\n",
    "\n",
    "def build_mini_resnet(input_size, num_classes):\n",
    "    inputs = layers.Input(shape=input_size)\n",
    "    x = layers.Conv2D(kernel_size=3, filters=32, strides=2, kernel_regularizer=keras.regularizers.l2(0.001))(inputs)\n",
    "    x = resnet_block(x, no_filters=32)\n",
    "    x = resnet_block(x, no_filters=32)\n",
    "    x = layers.Conv2D(kernel_size=3, filters=64, strides=2, kernel_regularizer=keras.regularizers.l2(0.001))(x)\n",
    "    x = resnet_block(x, no_filters=64)\n",
    "    x = resnet_block(x, no_filters=64)\n",
    "    x = layers.Conv2D(kernel_size=3, filters=128, strides=2, kernel_regularizer=keras.regularizers.l2(0.001))(x)\n",
    "    x = resnet_block(x, no_filters=128)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(num_classes)(x)\n",
    "    return keras.Model(inputs=inputs, outputs=x, name=\"mini_resnet\")\n",
    "\n",
    "\n",
    "def plot_history(history_to_plot):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history_to_plot.history['accuracy'], label='accuracy')\n",
    "    plt.plot(history_to_plot.history['val_accuracy'], label='val_accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history_to_plot.history['loss'], label='loss')\n",
    "    plt.plot(history_to_plot.history['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_model(model, name):\n",
    "    test_generator = DataGenerator(\"data/test.csv\", \"data/classes.csv\", BATCH_SIZE, INPUT_SHAPE)\n",
    "    val_loss, val_acc = model.evaluate(test_generator, verbose=2)\n",
    "    model.save(f\"./weights/acc_{str(val_acc)[:5]}_{name}\")\n",
    "\n",
    "\n",
    "train_set = DataGenerator(\"data/train.csv\", \"data/classes.csv\", BATCH_SIZE, INPUT_SHAPE)\n",
    "test_set = DataGenerator(\"data/test.csv\", \"data/classes.csv\", BATCH_SIZE, INPUT_SHAPE)\n",
    "\n",
    "\n",
    "def train(optimizer, name):\n",
    "    model = build_mini_resnet(INPUT_SHAPE_RGB, OUTPUTS)\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=optimizer,\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    history = model.fit(x=train_set, validation_data=test_set, batch_size=BATCH_SIZE, epochs=EPOCHS, shuffle=True)\n",
    "    save_model(model, name)\n",
    "    plot_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drsK8bes4J4z"
   },
   "source": [
    "## Training and fine-tuning\n",
    "\n",
    "Start by reading this blog [post](!http://karpathy.github.io/2019/04/25/recipe/), such that you can get an idea of the pipeline that you'll have to follow when training a model.\n",
    "\n",
    "- Triple check that your data loading is correct. (Analyse your data.)\n",
    "- Check that the setup is correct.\n",
    "- Overfit a simple network.\n",
    "- Add regularizations.\n",
    "  - data augmentation\n",
    "  - weight decay\n",
    "\n",
    "Finetune the learning rate. Use learning rate decay; here in the [documentation](!https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/LearningRateSchedule) you have an example on how you can use a learning rate scheduler in tensorflow.\n",
    "\n",
    "You should have at least 7 different trainings. Plot all the training history.\n",
    "\n",
    "__Save all your models and their training history!__ \n",
    "\n",
    "\n",
    "Create a google spreadsheet or a markdown table in this notebook, and report the configuration and the accuracy for all these trains. \n",
    "\n",
    "### Other useful videos (bias and variance, basic recipe for training a deep NN)\n",
    "- https://www.youtube.com/watch?v=NUmbgp1h64E \n",
    "- https://www.youtube.com/watch?v=SjQyLhQIXSM&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=2 \n",
    "- https://www.youtube.com/watch?v=C1N_PDHuJ6Q&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=3 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7T7MMSf9M4A"
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    train(keras.optimizers.Adam(1e-3), \"adam_1e3-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "                initial_learning_rate=1e-3,\n",
    "                decay_steps=5,\n",
    "                decay_rate=0.96)\n",
    "train(keras.optimizers.Adam(lr_schedule), \"adam_exp_decay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "                initial_learning_rate=5e-3,\n",
    "                decay_steps=10,\n",
    "                decay_rate=0.96)\n",
    "train(keras.optimizers.Adam(lr_schedule), \"adam_exp_decay_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYIv0nkd3IZK"
   },
   "source": [
    "## Ensembles\n",
    " \n",
    "Pick your N (3 or 5) of the networks that you've trained and create an ensemble. The prediction of the ensemble is just the average of the predictions of the N networks.\n",
    " \n",
    "Evaluate the ensemble (your accuracy should boost by at least 1.5%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbhzVNvC3mAd"
   },
   "outputs": [],
   "source": [
    "def make_ensemble(input_size, paths):\n",
    "    inputs = layers.Input(shape=input_size)\n",
    "    models = [keras.models.load_model(path) for path in paths]\n",
    "    for i, ensemble_model in enumerate(models):\n",
    "        ensemble_model._name += str(i)\n",
    "    x = layers.Average()([model(inputs) for model in models])\n",
    "    return keras.Model(inputs=inputs, outputs=x, name=\"ensemble\")\n",
    "\n",
    "\n",
    "model = make_ensemble(INPUT_SHAPE_RGB, [\n",
    "    \"weights/acc_0.649_adam_exp_decay_v2\",\n",
    "    \"weights/acc_0.634_adam_exp_decay_v2\",\n",
    "    \"weights/acc_0.658_adam_1e3-3\"\n",
    "\n",
    "])\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'],\n",
    "              )\n",
    "val_loss, val_acc = model.evaluate(test_set, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JC7WKQENUcDQ"
   },
   "source": [
    "# Transfer learning and fine-tuning\n",
    " \n",
    "In the _tensorflow.keras.applications_ module you can find implementations of several well known CNN architectures (most of the models that we covered during the lecture), as well as the pretrained weights of these models on the ImageNet dataset. \n",
    "You can use this module to apply transfer learning and fine-tuning for your classification problem. [Here](!https://keras.io/api/applications/) you can find a comprehensive table with the size of the models, number of parameters, top-1 and top-5 accuracy on the ImageNet dataset.\n",
    " \n",
    "When using deep neural networks, transfer learning is the norm, not the exception.  Transfer learning refers to the situation where what has been learned in one setting is used to improve generalization in another setting.\n",
    "The transfer learning pipeline can be summarized as follows:\n",
    "- get the weights of a model trained on similar classification problem (for which more training data is available);\n",
    "- remove the final classification layer;\n",
    "- freeze the weights (don't update them during the training process); these layers would be used as a feature extractor;\n",
    "- add a/some trainable layers over the frozen layers. They will learn how the extracted features can be used to distinguish between the classes of your classification problem.\n",
    "- train these new layers on your dataset.\n",
    " \n",
    "Next, you can also use fine-tuning. During fine-tuning you will unfreeze the model (or a larger part of the model), and train it on the new data with a very low learning rate.\n",
    " \n",
    "Follow this [tutorial](!https://keras.io/guides/transfer_learning/) to solve this exercise.\n",
    " \n",
    "When following the tutorial\n",
    "- pay attention to the discussion about the BatchNormalization layers;\n",
    "- you can skip the section \"Transfer learning & fine-tuning with a custom training loop\", we'll cover this in the next laboratory;\n",
    "- pay attention to the loss that you will be using when training your model. In the tutorial the loss is the binary cross entropy loss which is suitable for binary classification problems. If your problem is multi-class you should use the categorical cross entropy loss.\n",
    "- use the pre-processing required by the network architecture that you chose.\n",
    " \n",
    "To sum up, pick a neural network architecture from the _tensorflow.keras.applications_ module and use transfer learning and fine tuning to train it to classify the images from your dataset (you should use the custom DataGenerator that you wrote for this). \n",
    " Briefly describe the key features of the neural network architecture that you chose and why you chose it.\n",
    " \n",
    "Apply transfer learning (with at least one config for the hyperparameters) and report the performance. Apply fine-tuning  (with at least one config for the hyperparameters) and report the performance.\n",
    "Finally, plot the performance of the model when you used only transfer learning and the performance of the model when you also used fine-tuning on the same plot.\n",
    " \n",
    "I chose the architecture <font color='red'> TODO </font> , because <font color='red'> TODO </font> .\n",
    "The key features of this architecture are\n",
    "- <font color='red'> TODO  </font> \n",
    "- <font color='red'> TODO  </font> \n",
    "- <font color='red'> TODO  </font> \n",
    " \n",
    "How does the performance of this fine-tuned model compare to the performance of the network that you trained from scratch?\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMe6VdYWQpgu"
   },
   "outputs": [],
   "source": [
    "# TODO your transfer-learning and fine-tuning step"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Computer Vision and Deep Learning - Laboratory 4.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a84fd92db5a471e91f372cecb47adab54dcdef527b642916ca90b59569abdda0"
  },
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
